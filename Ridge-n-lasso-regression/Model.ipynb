{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perpare the Model (Lasso Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    def __init__(self, learning_rate, iterations, regularization_parameter):\n",
    "        self.m = None\n",
    "        self.n = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.regularization_parameter = regularization_parameter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def cost_function(self, y, y_pred):\n",
    "        return (1/(2*self.m))*np.sum(np.square(y-y_pred)) + self.regularization_parameter(self.w)\n",
    "        \n",
    "    def hypothesis(self, weights, bias, X):\n",
    "        return X.dot(weights) + bias\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        try:\n",
    "            y.shape[1]\n",
    "        except IndexError as e:\n",
    "            print(\"ERROR: Target array should be a one dimensional array not a list\"\n",
    "                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n",
    "                  .format(shape_y_0=y.shape[0], shape_y=y.shape))\n",
    "            return\n",
    "        self.m = X.shape[0]\n",
    "        self.n = X.shape[1]\n",
    "        self.w = np.zeros((self.n, 1))\n",
    "        self.b = 0\n",
    "\n",
    "        for it in range(1, self.iterations + 1):\n",
    "            y_pred = self.hypothesis(self.w, self.b, X)\n",
    "            cost = self.cost_function(y, y_pred)\n",
    "            dw = (1/self.m) * np.dot(X.T, (y_pred - y)) + self.regularization_parameter.derivation(self.w)\n",
    "            self.w = self.w - self.learning_rate * dw\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                print(\"The Cost function for the iteration {}----->{} :)\".format(it, cost))\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        test_X = np.insert(test_X, 0, 1, axis=1)\n",
    "        y_pred = self.hypothesis(self.w, self.b, test_X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class l1_regularization:\n",
    "    def __init__(self, lamda):\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return self.lamda * np.sum(np.abs(weights))\n",
    "    \n",
    "    def derivation(self, weights):\n",
    "        return self.lamda * np.sign(weights)\n",
    "    \n",
    "\n",
    "class l2_regularization:\n",
    "    def __init__(self, lamda):\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return self.lamda * np.sum(np.square(weights))\n",
    "    \n",
    "    def derivation(self, weights):\n",
    "        return self.lamda * 2 * (weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data samples-----> 80000\n",
      "Number of training features --------> 8\n",
      "Shape of the target value ----------> (80000, 1)\n"
     ]
    }
   ],
   "source": [
    "X,y=make_regression(n_samples=80000, n_features=8)\n",
    "\n",
    "y=y[:,np.newaxis]\n",
    "\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))\n",
    "print(\"Shape of the target value ----------> {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.115864</td>\n",
       "      <td>0.874499</td>\n",
       "      <td>1.586169</td>\n",
       "      <td>-1.483878</td>\n",
       "      <td>0.200694</td>\n",
       "      <td>0.383530</td>\n",
       "      <td>-0.917942</td>\n",
       "      <td>-0.751221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.827163</td>\n",
       "      <td>-1.957370</td>\n",
       "      <td>0.583514</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.239281</td>\n",
       "      <td>1.085531</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>-0.405198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.933050</td>\n",
       "      <td>1.229018</td>\n",
       "      <td>-1.716412</td>\n",
       "      <td>-0.134172</td>\n",
       "      <td>-1.129719</td>\n",
       "      <td>-0.523720</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.042619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.192906</td>\n",
       "      <td>0.275112</td>\n",
       "      <td>-0.231609</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.122628</td>\n",
       "      <td>0.787526</td>\n",
       "      <td>0.445755</td>\n",
       "      <td>-2.056910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.053677</td>\n",
       "      <td>1.799800</td>\n",
       "      <td>-1.151786</td>\n",
       "      <td>2.315823</td>\n",
       "      <td>-1.480508</td>\n",
       "      <td>0.619357</td>\n",
       "      <td>-0.039033</td>\n",
       "      <td>0.245762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2  ...         5         6         7\n",
       "0 -1.115864  0.874499  1.586169  ...  0.383530 -0.917942 -0.751221\n",
       "1 -1.827163 -1.957370  0.583514  ...  1.085531  0.004975 -0.405198\n",
       "2  1.933050  1.229018 -1.716412  ... -0.523720  0.152250  0.042619\n",
       "3  3.192906  0.275112 -0.231609  ...  0.787526  0.445755 -2.056910\n",
       "4  1.053677  1.799800 -1.151786  ...  0.619357 -0.039033  0.245762\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(X)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.468766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-83.965764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.592487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.168922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>227.958708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0  -75.468766\n",
       "1  -83.965764\n",
       "2   41.592487\n",
       "3   90.168922\n",
       "4  227.958708"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y=pd.DataFrame(y)\n",
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso_Regression(Regression):\n",
    "    def __init__(self,lamda,learning_rate,iterations):\n",
    "        self.regularization = l1_regularization(lamda)\n",
    "        super(Lasso_Regression, self).__init__(learning_rate, iterations, self.regularization)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        return super(Lasso_Regression, self).train(X, y)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        return super(Lasso_Regression, self).predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the parameters and Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cost function for the iteration 10----->12832.992924657494 :)\n",
      "The Cost function for the iteration 20----->10505.044357439901 :)\n",
      "The Cost function for the iteration 30----->8599.561615591605 :)\n",
      "The Cost function for the iteration 40----->7039.8741102536505 :)\n",
      "The Cost function for the iteration 50----->5763.22631407234 :)\n",
      "The Cost function for the iteration 60----->4718.25218381324 :)\n",
      "The Cost function for the iteration 70----->3862.9079945251237 :)\n",
      "The Cost function for the iteration 80----->3162.7803763412185 :)\n",
      "The Cost function for the iteration 90----->2589.7014494118125 :)\n",
      "The Cost function for the iteration 100----->2120.615314934108 :)\n",
      "The Cost function for the iteration 110----->1736.6502785540129 :)\n",
      "The Cost function for the iteration 120----->1422.3594639625835 :)\n",
      "The Cost function for the iteration 130----->1165.0992527362318 :)\n",
      "The Cost function for the iteration 140----->954.5205342800864 :)\n",
      "The Cost function for the iteration 150----->782.1522904890837 :)\n",
      "The Cost function for the iteration 160----->641.0607562533497 :)\n",
      "The Cost function for the iteration 170----->525.5704388276324 :)\n",
      "The Cost function for the iteration 180----->431.0357688207528 :)\n",
      "The Cost function for the iteration 190----->353.6541933710181 :)\n",
      "The Cost function for the iteration 200----->290.31318999262305 :)\n",
      "The Cost function for the iteration 210----->238.46504474967543 :)\n",
      "The Cost function for the iteration 220----->196.02435579335986 :)\n",
      "The Cost function for the iteration 230----->161.28413786288928 :)\n",
      "The Cost function for the iteration 240----->132.84715191632824 :)\n",
      "The Cost function for the iteration 250----->109.56969675451074 :)\n",
      "The Cost function for the iteration 260----->90.51560099199554 :)\n",
      "The Cost function for the iteration 270----->74.91856419848732 :)\n",
      "The Cost function for the iteration 280----->62.15133200365043 :)\n",
      "The Cost function for the iteration 290----->51.70046495000507 :)\n",
      "The Cost function for the iteration 300----->43.14568596051557 :)\n",
      "The Cost function for the iteration 310----->36.14297551830813 :)\n",
      "The Cost function for the iteration 320----->30.410735769669976 :)\n",
      "The Cost function for the iteration 330----->25.718462793939956 :)\n",
      "The Cost function for the iteration 340----->21.87747280310802 :)\n",
      "The Cost function for the iteration 350----->18.733315711983494 :)\n",
      "The Cost function for the iteration 360----->16.159568585989213 :)\n",
      "The Cost function for the iteration 370----->14.052743035404637 :)\n",
      "The Cost function for the iteration 380----->12.328127922929749 :)\n",
      "The Cost function for the iteration 390----->10.916381475006148 :)\n",
      "The Cost function for the iteration 400----->9.76074270241873 :)\n",
      "The Cost function for the iteration 410----->8.814749555481201 :)\n",
      "The Cost function for the iteration 420----->8.040367443073254 :)\n",
      "The Cost function for the iteration 430----->7.406462899396284 :)\n",
      "The Cost function for the iteration 440----->6.887553413255495 :)\n",
      "The Cost function for the iteration 450----->6.4627746794784535 :)\n",
      "The Cost function for the iteration 460----->6.115052900765413 :)\n",
      "The Cost function for the iteration 470----->5.830406546074082 :)\n",
      "The Cost function for the iteration 480----->5.597396130604876 :)\n",
      "The Cost function for the iteration 490----->5.406652240299058 :)\n",
      "The Cost function for the iteration 500----->5.250508313078387 :)\n",
      "The Cost function for the iteration 510----->5.12268875593241 :)\n",
      "The Cost function for the iteration 520----->5.018054186384255 :)\n",
      "The Cost function for the iteration 530----->4.932398816595765 :)\n",
      "The Cost function for the iteration 540----->4.862280815556854 :)\n",
      "The Cost function for the iteration 550----->4.804881710025808 :)\n",
      "The Cost function for the iteration 560----->4.757893978807631 :)\n",
      "The Cost function for the iteration 570----->4.7194284893753276 :)\n",
      "The Cost function for the iteration 580----->4.6879398805968515 :)\n",
      "The Cost function for the iteration 590----->4.662162698014222 :)\n",
      "The Cost function for the iteration 600----->4.6410612709434345 :)\n",
      "The Cost function for the iteration 610----->4.623787334378482 :)\n",
      "The Cost function for the iteration 620----->4.60964647535924 :)\n",
      "The Cost function for the iteration 630----->4.598070404154452 :)\n",
      "The Cost function for the iteration 640----->4.588593913817397 :)\n",
      "The Cost function for the iteration 650----->4.5808359869947015 :)\n",
      "The Cost function for the iteration 660----->4.574484992127596 :)\n",
      "The Cost function for the iteration 670----->4.569285860127764 :)\n",
      "The Cost function for the iteration 680----->4.565029673169006 :)\n",
      "The Cost function for the iteration 690----->4.561545407215108 :)\n",
      "The Cost function for the iteration 700----->4.558693058503103 :)\n",
      "The Cost function for the iteration 710----->4.556358016859912 :)\n",
      "The Cost function for the iteration 720----->4.55444645967124 :)\n",
      "The Cost function for the iteration 730----->4.5528815813537 :)\n",
      "The Cost function for the iteration 740----->4.551600506769017 :)\n",
      "The Cost function for the iteration 750----->4.550551764513711 :)\n",
      "The Cost function for the iteration 760----->4.549693218524167 :)\n",
      "The Cost function for the iteration 770----->4.548990442600209 :)\n",
      "The Cost function for the iteration 780----->4.5484151588905375 :)\n",
      "The Cost function for the iteration 790----->4.547944206239455 :)\n",
      "The Cost function for the iteration 800----->4.547558662841638 :)\n",
      "The Cost function for the iteration 810----->4.5472430387142095 :)\n",
      "The Cost function for the iteration 820----->4.546984653252761 :)\n",
      "The Cost function for the iteration 830----->4.54677312568636 :)\n",
      "The Cost function for the iteration 840----->4.54659995795316 :)\n",
      "The Cost function for the iteration 850----->4.546458193232916 :)\n",
      "The Cost function for the iteration 860----->4.546342136413463 :)\n",
      "The Cost function for the iteration 870----->4.546247125257486 :)\n",
      "The Cost function for the iteration 880----->4.546169343073493 :)\n",
      "The Cost function for the iteration 890----->4.546105665363004 :)\n",
      "The Cost function for the iteration 900----->4.546053534281439 :)\n",
      "The Cost function for the iteration 910----->4.546010855867962 :)\n",
      "The Cost function for the iteration 920----->4.54597591591452 :)\n",
      "The Cost function for the iteration 930----->4.545947311093488 :)\n",
      "The Cost function for the iteration 940----->4.545923892576367 :)\n",
      "The Cost function for the iteration 950----->4.545904719878074 :)\n",
      "The Cost function for the iteration 960----->4.545889023072169 :)\n",
      "The Cost function for the iteration 970----->4.545876171858831 :)\n",
      "The Cost function for the iteration 980----->4.545865650242702 :)\n",
      "The Cost function for the iteration 990----->4.545857035803166 :)\n",
      "The Cost function for the iteration 1000----->4.5458499827241425 :)\n",
      "The r2_score of the trained model 0.9999999585874288\n"
     ]
    }
   ],
   "source": [
    "param={\n",
    "    \"lamda\":0.01,\n",
    "    \"learning_rate\":0.01,\n",
    "    \"iterations\":1000\n",
    "}\n",
    "\n",
    "linear_reg=Lasso_Regression(**param)\n",
    "\n",
    "linear_reg.train(X,y)\n",
    "\n",
    "y_pred=linear_reg.predict(X)\n",
    "\n",
    "score = r2_score(y, y_pred)\n",
    "print(\"The r2_score of the trained model\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets verify from pre defined library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data samples-----> 80000\n",
      "Number of training features --------> 8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r2_score of the trained model 0.999738756695463\n"
     ]
    }
   ],
   "source": [
    "lasso_sklearn = Lasso()\n",
    "lasso_sklearn.fit(X, y)\n",
    "\n",
    "y_pred_sklearn = lasso_sklearn.predict(X)\n",
    "\n",
    "score_sklearn = r2_score(y, y_pred_sklearn)\n",
    "print(\"The r2_score of the trained model\", score_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perpare the Model (Ridge Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ridge_Regression(Regression):\n",
    "    def __init__(self,lamda,learning_rate,iterations):\n",
    "        self.regularization = l2_regularization(lamda)\n",
    "        super(Ridge_Regression, self).__init__(learning_rate, iterations, self.regularization)\n",
    "\n",
    "        def train(self, X, y):\n",
    "            return super(Ridge_Regression, self).train(X, y)\n",
    "        \n",
    "        def predict(self, test_X):\n",
    "            return super(Ridge_Regression, self).predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the parameters and Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cost function for the iteration 10----->12836.490214718964 :)\n",
      "The Cost function for the iteration 20----->10520.590270697554 :)\n",
      "The Cost function for the iteration 30----->8632.613767831617 :)\n",
      "The Cost function for the iteration 40----->7093.487279430373 :)\n",
      "The Cost function for the iteration 50----->5838.7495023387955 :)\n",
      "The Cost function for the iteration 60----->4815.850941493497 :)\n",
      "The Cost function for the iteration 70----->3981.9526330924414 :)\n",
      "The Cost function for the iteration 80----->3302.131676341332 :)\n",
      "The Cost function for the iteration 90----->2747.918390006724 :)\n",
      "The Cost function for the iteration 100----->2296.1038055884105 :)\n",
      "The Cost function for the iteration 110----->1927.7675361916154 :)\n",
      "The Cost function for the iteration 120----->1627.485293854467 :)\n",
      "The Cost function for the iteration 130----->1382.6828551386498 :)\n",
      "The Cost function for the iteration 140----->1183.109410662881 :)\n",
      "The Cost function for the iteration 150----->1020.4082360847535 :)\n",
      "The Cost function for the iteration 160----->887.7666994225706 :)\n",
      "The Cost function for the iteration 170----->779.6309434177482 :)\n",
      "The Cost function for the iteration 180----->691.473291151404 :)\n",
      "The Cost function for the iteration 190----->619.6026318829835 :)\n",
      "The Cost function for the iteration 200----->561.0098446259077 :)\n",
      "The Cost function for the iteration 210----->513.2417847606566 :)\n",
      "The Cost function for the iteration 220----->474.2985555101746 :)\n",
      "The Cost function for the iteration 230----->442.54976149982735 :)\n",
      "The Cost function for the iteration 240----->416.66623676214203 :)\n",
      "The Cost function for the iteration 250----->395.5643877396404 :)\n",
      "The Cost function for the iteration 260----->378.3608202444635 :)\n",
      "The Cost function for the iteration 270----->364.3353500891808 :)\n",
      "The Cost function for the iteration 280----->352.9008482556866 :)\n",
      "The Cost function for the iteration 290----->343.5786577296137 :)\n",
      "The Cost function for the iteration 300----->335.9785524885998 :)\n",
      "The Cost function for the iteration 310----->329.78239937005833 :)\n",
      "The Cost function for the iteration 320----->324.7308386271169 :)\n",
      "The Cost function for the iteration 330----->320.6124254066015 :)\n",
      "The Cost function for the iteration 340----->317.2547774462745 :)\n",
      "The Cost function for the iteration 350----->314.517358307134 :)\n",
      "The Cost function for the iteration 360----->312.28559394984956 :)\n",
      "The Cost function for the iteration 370----->310.46607630127005 :)\n",
      "The Cost function for the iteration 380----->308.9826529762063 :)\n",
      "The Cost function for the iteration 390----->307.773239427933 :)\n",
      "The Cost function for the iteration 400----->306.78722005231907 :)\n",
      "The Cost function for the iteration 410----->305.9833294322183 :)\n",
      "The Cost function for the iteration 420----->305.32792501362803 :)\n",
      "The Cost function for the iteration 430----->304.7935788951693 :)\n",
      "The Cost function for the iteration 440----->304.35792977408994 :)\n",
      "The Cost function for the iteration 450----->304.00274698482224 :)\n",
      "The Cost function for the iteration 460----->303.7131674463202 :)\n",
      "The Cost function for the iteration 470----->303.47707357384064 :)\n",
      "The Cost function for the iteration 480----->303.28458611268746 :)\n",
      "The Cost function for the iteration 490----->303.1276506628532 :)\n",
      "The Cost function for the iteration 500----->302.99970058595306 :)\n",
      "The Cost function for the iteration 510----->302.89538218359274 :)\n",
      "The Cost function for the iteration 520----->302.8103306432603 :)\n",
      "The Cost function for the iteration 530----->302.7409873731269 :)\n",
      "The Cost function for the iteration 540----->302.68445107978783 :)\n",
      "The Cost function for the iteration 550----->302.6383563555042 :)\n",
      "The Cost function for the iteration 560----->302.6007746930806 :)\n",
      "The Cost function for the iteration 570----->302.57013378532673 :)\n",
      "The Cost function for the iteration 580----->302.5451517314295 :)\n",
      "The Cost function for the iteration 590----->302.52478339653834 :)\n",
      "The Cost function for the iteration 600----->302.5081766795652 :)\n",
      "The Cost function for the iteration 610----->302.4946368589262 :)\n",
      "The Cost function for the iteration 620----->302.4835975240563 :)\n",
      "The Cost function for the iteration 630----->302.4745968761786 :)\n",
      "The Cost function for the iteration 640----->302.46725840652897 :)\n",
      "The Cost function for the iteration 650----->302.46127514344937 :)\n",
      "The Cost function for the iteration 660----->302.45639680913007 :)\n",
      "The Cost function for the iteration 670----->302.4524193485494 :)\n",
      "The Cost function for the iteration 680----->302.4491763924406 :)\n",
      "The Cost function for the iteration 690----->302.4465322970525 :)\n",
      "The Cost function for the iteration 700----->302.44437646945806 :)\n",
      "The Cost function for the iteration 710----->302.4426187409623 :)\n",
      "The Cost function for the iteration 720----->302.44118559502084 :)\n",
      "The Cost function for the iteration 730----->302.4400170918381 :)\n",
      "The Cost function for the iteration 740----->302.43906436096944 :)\n",
      "The Cost function for the iteration 750----->302.4382875570139 :)\n",
      "The Cost function for the iteration 760----->302.4376541928699 :)\n",
      "The Cost function for the iteration 770----->302.43713778081604 :)\n",
      "The Cost function for the iteration 780----->302.4367167245649 :)\n",
      "The Cost function for the iteration 790----->302.4363734159369 :)\n",
      "The Cost function for the iteration 800----->302.43609349836265 :)\n",
      "The Cost function for the iteration 810----->302.43586526640166 :)\n",
      "The Cost function for the iteration 820----->302.4356791761581 :)\n",
      "The Cost function for the iteration 830----->302.43552744611105 :)\n",
      "The Cost function for the iteration 840----->302.43540373166206 :)\n",
      "The Cost function for the iteration 850----->302.43530285978517 :)\n",
      "The Cost function for the iteration 860----->302.4352206126801 :)\n",
      "The Cost function for the iteration 870----->302.4351535513781 :)\n",
      "The Cost function for the iteration 880----->302.43509887192266 :)\n",
      "The Cost function for the iteration 890----->302.43505428810835 :)\n",
      "The Cost function for the iteration 900----->302.4350179358749 :)\n",
      "The Cost function for the iteration 910----->302.4349882953552 :)\n",
      "The Cost function for the iteration 920----->302.434964127319 :)\n",
      "The Cost function for the iteration 930----->302.43494442135204 :)\n",
      "The Cost function for the iteration 940----->302.4349283536057 :)\n",
      "The Cost function for the iteration 950----->302.4349152523471 :)\n",
      "The Cost function for the iteration 960----->302.4349045698716 :)\n",
      "The Cost function for the iteration 970----->302.4348958596016 :)\n",
      "The Cost function for the iteration 980----->302.4348887574136 :)\n",
      "The Cost function for the iteration 990----->302.4348829664141 :)\n",
      "The Cost function for the iteration 1000----->302.4348782445256 :)\n",
      "The r2_score of the trained model 0.9996111971429361\n"
     ]
    }
   ],
   "source": [
    "param={\n",
    "    \"lamda\":0.01,\n",
    "    \"learning_rate\":0.01,\n",
    "    \"iterations\":1000\n",
    "}\n",
    "\n",
    "linear_reg=Ridge_Regression(**param)\n",
    "\n",
    "linear_reg.train(X,y)\n",
    "\n",
    "y_pred=linear_reg.predict(X)\n",
    "\n",
    "score = r2_score(y, y_pred)\n",
    "print(\"The r2_score of the trained model\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets verify from pre defined library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r2_score of the trained model 0.9999999998425447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_sklearn = Ridge()\n",
    "ridge_sklearn.fit(X, y)\n",
    "\n",
    "y_pred_sklearn = ridge_sklearn.predict(X)\n",
    "\n",
    "score_sklearn = r2_score(y, y_pred_sklearn)\n",
    "print(\"The r2_score of the trained model\", score_sklearn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
