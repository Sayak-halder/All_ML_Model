{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11925879,"sourceType":"datasetVersion","datasetId":7498046}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:55:06.601475Z","iopub.execute_input":"2025-05-23T20:55:06.602017Z","iopub.status.idle":"2025-05-23T20:55:07.469964Z","shell.execute_reply.started":"2025-05-23T20:55:06.601992Z","shell.execute_reply":"2025-05-23T20:55:07.469191Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/brain-tumour/brain_tumor_dataset/no/34 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N20.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N1.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/49 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N15.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No18.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/31 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 6.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/3 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 5.jpeg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 1.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/26 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N26.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N5.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/6 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No21.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/17 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No22.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/29 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/46 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/32 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/42 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 90.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 94.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No19.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/47 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N22.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No12.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/15 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/20 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No13.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/11 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N11.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No17.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 98.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/18 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N19.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/25 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/24 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 91.jpeg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/45 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/22 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No16.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/44no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/4 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/36 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/8 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/48 no.jpeg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No20.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 2.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 10.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/7 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/10 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N17.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/14 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 8.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/38 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/40 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 9.png\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N3.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 89.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/19 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/12 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N2.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No11.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 92.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No14.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/39 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 7.jpeg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/27 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/5 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 95.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 3.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/No15.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 923.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/28 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 100.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/23 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/13 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/37 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/21 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/30 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 4.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/9 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 99.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N21.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/50 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/43 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 96.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/41 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N16.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/no 97.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/1 no.jpeg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/2 no.jpeg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/35 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/N6.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/no/33 no.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y115.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y192.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y162.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y9.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y108.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y155.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y106.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y258.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y103.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y185.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y180.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y120.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y54.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y165.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y112.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y99.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y3.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y184.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y82.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y169.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y146.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y147.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y116.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y27.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y73.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y181.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y81.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y161.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y17.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y69.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y158.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y32.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y186.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y259.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y42.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y250.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y58.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y60.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y252.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y182.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y154.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y79.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y31.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y98.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y91.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y95.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y22.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y35.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y33.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y247.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y19.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y160.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y13.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y29.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y55.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y4.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y50.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y114.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y26.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y257.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y97.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y14.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y25.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y170.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y51.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y34.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y39.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y15.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y104.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y187.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y167.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y40.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y38.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y111.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y37.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y24.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y157.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y6.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y28.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y164.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y30.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y90.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y92.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y86.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y46.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y248.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y1.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y2.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y62.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y67.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y109.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y117.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y194.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y71.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y7.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y59.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y246.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y66.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y148.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y243.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y56.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y18.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y23.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y96.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y195.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y65.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y16.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y193.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y45.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y53.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y245.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y159.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y12.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y100.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y102.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y75.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y256.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y10.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y253.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y105.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y166.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y61.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y47.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y255.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y70.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y168.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y163.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y188.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y242.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y21.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y36.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y254.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y183.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y74.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y251.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y52.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y153.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y156.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y11.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y77.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y249.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y41.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y76.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y78.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y20.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y85.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y49.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y244.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y8.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y107.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y113.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y101.jpg\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y89.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y44.JPG\n/kaggle/input/brain-tumour/brain_tumor_dataset/yes/Y92.png\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport kagglehub\nimport tensorflow as tf\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D as GAP\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:55:12.287955Z","iopub.execute_input":"2025-05-23T20:55:12.288572Z","iopub.status.idle":"2025-05-23T20:55:25.315391Z","shell.execute_reply.started":"2025-05-23T20:55:12.288546Z","shell.execute_reply":"2025-05-23T20:55:25.314625Z"}},"outputs":[{"name":"stderr","text":"2025-05-23 20:55:13.880418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748033714.061060      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748033714.111417      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom pathlib import Path\n\nsource_dir = Path(\"/kaggle/input/brain-tumour/brain_tumor_dataset\")\noutput_dir = Path(\"/kaggle/working/dataset\")\n\ntrain_split = 0.7\nval_split = 0.15\ntest_split = 0.15\n\nclasses = ['yes', 'no']\nvalid_extensions = ['.jpg', '.JPG', '.png']\n\nfor split in ['train', 'val', 'test']:\n    for cls in classes:\n        (output_dir / split / cls).mkdir(parents=True, exist_ok=True)\n\ndef split_data(class_name):\n    src = source_dir / class_name\n    all_files = [f for f in src.iterdir() if f.suffix.lower() in valid_extensions]\n    random.shuffle(all_files)\n\n    total = len(all_files)\n    train_end = int(total * train_split)\n    val_end = train_end + int(total * val_split)\n\n    train_files = all_files[:train_end]\n    val_files = all_files[train_end:val_end]\n    test_files = all_files[val_end:]\n\n    for files, split in zip([train_files, val_files, test_files], ['train', 'val', 'test']):\n        for f in files:\n            dest = output_dir / split / class_name / f.name\n            shutil.copy(f, dest)\n\nfor cls in classes:\n    split_data(cls)\n\nprint(\"âœ… Dataset split into train/val/test successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:57:32.252908Z","iopub.execute_input":"2025-05-23T20:57:32.253587Z","iopub.status.idle":"2025-05-23T20:57:33.600948Z","shell.execute_reply.started":"2025-05-23T20:57:32.253558Z","shell.execute_reply":"2025-05-23T20:57:33.600319Z"}},"outputs":[{"name":"stdout","text":"âœ… Dataset split into train/val/test successfully.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n\ntrain_dir = '/kaggle/working/dataset/train'\nval_dir = '/kaggle/working/dataset/val'\ntest_dir = '/kaggle/working/dataset/test'\n\nIMG_SIZE = 224\nBATCH_SIZE = 16\n\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std = [0.229, 0.224, 0.225]\n\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),                    \n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:57:52.779230Z","iopub.execute_input":"2025-05-23T20:57:52.779549Z","iopub.status.idle":"2025-05-23T20:57:52.784735Z","shell.execute_reply.started":"2025-05-23T20:57:52.779527Z","shell.execute_reply":"2025-05-23T20:57:52.783885Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_dataset = datasets.ImageFolder(root='dataset/train', transform=transform)\nval_dataset   = datasets.ImageFolder(root='dataset/val', transform=transform)\ntest_dataset  = datasets.ImageFolder(root='dataset/test', transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\nprint(\"Classes:\", train_dataset.classes)\nprint(\"Label mapping:\", train_dataset.class_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:57:57.703940Z","iopub.execute_input":"2025-05-23T20:57:57.704209Z","iopub.status.idle":"2025-05-23T20:57:57.711449Z","shell.execute_reply.started":"2025-05-23T20:57:57.704188Z","shell.execute_reply":"2025-05-23T20:57:57.710742Z"}},"outputs":[{"name":"stdout","text":"Classes: ['no', 'yes']\nLabel mapping: {'no': 0, 'yes': 1}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_name = 'resnet50'\n\nmodel = models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 1)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:58:01.220366Z","iopub.execute_input":"2025-05-23T20:58:01.220654Z","iopub.status.idle":"2025-05-23T20:58:02.536471Z","shell.execute_reply.started":"2025-05-23T20:58:01.220635Z","shell.execute_reply":"2025-05-23T20:58:02.535664Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 205MB/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\nnum_epochs = 20\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.float().unsqueeze(1).to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            preds = torch.sigmoid(outputs).squeeze() > 0.5\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    val_acc = 100 * correct / total\n    print(f\"Validation Accuracy: {val_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:58:04.299153Z","iopub.execute_input":"2025-05-23T20:58:04.299868Z","iopub.status.idle":"2025-05-23T20:58:40.820110Z","shell.execute_reply.started":"2025-05-23T20:58:04.299841Z","shell.execute_reply":"2025-05-23T20:58:40.819388Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/20], Loss: 6.7927\nValidation Accuracy: 83.33%\nEpoch [2/20], Loss: 5.2782\nValidation Accuracy: 83.33%\nEpoch [3/20], Loss: 3.9422\nValidation Accuracy: 80.56%\nEpoch [4/20], Loss: 3.3304\nValidation Accuracy: 80.56%\nEpoch [5/20], Loss: 2.4583\nValidation Accuracy: 80.56%\nEpoch [6/20], Loss: 1.8839\nValidation Accuracy: 77.78%\nEpoch [7/20], Loss: 1.2790\nValidation Accuracy: 77.78%\nEpoch [8/20], Loss: 1.3258\nValidation Accuracy: 77.78%\nEpoch [9/20], Loss: 0.7078\nValidation Accuracy: 83.33%\nEpoch [10/20], Loss: 0.8280\nValidation Accuracy: 88.89%\nEpoch [11/20], Loss: 0.6016\nValidation Accuracy: 80.56%\nEpoch [12/20], Loss: 0.4277\nValidation Accuracy: 80.56%\nEpoch [13/20], Loss: 0.3093\nValidation Accuracy: 80.56%\nEpoch [14/20], Loss: 0.5749\nValidation Accuracy: 83.33%\nEpoch [15/20], Loss: 0.3832\nValidation Accuracy: 83.33%\nEpoch [16/20], Loss: 0.2208\nValidation Accuracy: 83.33%\nEpoch [17/20], Loss: 0.2491\nValidation Accuracy: 83.33%\nEpoch [18/20], Loss: 0.1855\nValidation Accuracy: 83.33%\nEpoch [19/20], Loss: 0.2534\nValidation Accuracy: 83.33%\nEpoch [20/20], Loss: 0.1649\nValidation Accuracy: 83.33%\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        preds = torch.sigmoid(outputs).squeeze() > 0.5\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"ðŸ§ª Test Accuracy: {100 * correct / total:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:58:45.211947Z","iopub.execute_input":"2025-05-23T20:58:45.212627Z","iopub.status.idle":"2025-05-23T20:58:45.466821Z","shell.execute_reply.started":"2025-05-23T20:58:45.212588Z","shell.execute_reply":"2025-05-23T20:58:45.466043Z"}},"outputs":[{"name":"stdout","text":"ðŸ§ª Test Accuracy: 84.62%\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_name = 'resnet18'\n\nmodel = models.resnet18(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 1)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:59:39.650227Z","iopub.execute_input":"2025-05-23T20:59:39.651121Z","iopub.status.idle":"2025-05-23T20:59:40.112141Z","shell.execute_reply.started":"2025-05-23T20:59:39.651084Z","shell.execute_reply":"2025-05-23T20:59:40.111493Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 233MB/s]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\nnum_epochs = 20\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.float().unsqueeze(1).to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            preds = torch.sigmoid(outputs).squeeze() > 0.5\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    val_acc = 100 * correct / total\n    print(f\"Validation Accuracy: {val_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T20:59:42.883887Z","iopub.execute_input":"2025-05-23T20:59:42.884153Z","iopub.status.idle":"2025-05-23T21:00:03.953981Z","shell.execute_reply.started":"2025-05-23T20:59:42.884131Z","shell.execute_reply":"2025-05-23T21:00:03.953300Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/20], Loss: 6.5383\nValidation Accuracy: 50.00%\nEpoch [2/20], Loss: 4.9966\nValidation Accuracy: 69.44%\nEpoch [3/20], Loss: 4.0221\nValidation Accuracy: 77.78%\nEpoch [4/20], Loss: 3.3540\nValidation Accuracy: 83.33%\nEpoch [5/20], Loss: 2.6752\nValidation Accuracy: 83.33%\nEpoch [6/20], Loss: 2.3800\nValidation Accuracy: 83.33%\nEpoch [7/20], Loss: 1.9552\nValidation Accuracy: 86.11%\nEpoch [8/20], Loss: 1.5375\nValidation Accuracy: 86.11%\nEpoch [9/20], Loss: 1.5739\nValidation Accuracy: 88.89%\nEpoch [10/20], Loss: 1.0515\nValidation Accuracy: 88.89%\nEpoch [11/20], Loss: 1.1590\nValidation Accuracy: 91.67%\nEpoch [12/20], Loss: 1.0442\nValidation Accuracy: 86.11%\nEpoch [13/20], Loss: 0.8324\nValidation Accuracy: 88.89%\nEpoch [14/20], Loss: 0.5734\nValidation Accuracy: 91.67%\nEpoch [15/20], Loss: 0.6386\nValidation Accuracy: 91.67%\nEpoch [16/20], Loss: 0.5580\nValidation Accuracy: 91.67%\nEpoch [17/20], Loss: 0.4984\nValidation Accuracy: 91.67%\nEpoch [18/20], Loss: 0.3624\nValidation Accuracy: 91.67%\nEpoch [19/20], Loss: 0.3888\nValidation Accuracy: 91.67%\nEpoch [20/20], Loss: 0.5132\nValidation Accuracy: 91.67%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        preds = torch.sigmoid(outputs).squeeze() > 0.5\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"ðŸ§ª Test Accuracy: {100 * correct / total:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T21:00:52.080367Z","iopub.execute_input":"2025-05-23T21:00:52.080647Z","iopub.status.idle":"2025-05-23T21:00:52.246034Z","shell.execute_reply.started":"2025-05-23T21:00:52.080629Z","shell.execute_reply":"2025-05-23T21:00:52.245291Z"}},"outputs":[{"name":"stdout","text":"ðŸ§ª Test Accuracy: 84.62%\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model = models.resnet18(pretrained=False)\nmodel.fc = nn.Linear(model.fc.in_features, 1)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T21:04:14.201728Z","iopub.execute_input":"2025-05-23T21:04:14.202269Z","iopub.status.idle":"2025-05-23T21:04:14.369147Z","shell.execute_reply.started":"2025-05-23T21:04:14.202229Z","shell.execute_reply":"2025-05-23T21:04:14.368475Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nnum_epochs = 20\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.float().unsqueeze(1).to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            preds = torch.sigmoid(outputs).squeeze() > 0.5\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    val_acc = 100 * correct / total\n    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T21:04:43.425589Z","iopub.execute_input":"2025-05-23T21:04:43.425855Z","iopub.status.idle":"2025-05-23T21:05:03.978295Z","shell.execute_reply.started":"2025-05-23T21:04:43.425836Z","shell.execute_reply":"2025-05-23T21:05:03.977596Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/20], Loss: 6.4278\nValidation Accuracy: 80.56%\nEpoch [2/20], Loss: 4.3366\nValidation Accuracy: 63.89%\nEpoch [3/20], Loss: 2.3491\nValidation Accuracy: 69.44%\nEpoch [4/20], Loss: 1.2533\nValidation Accuracy: 88.89%\nEpoch [5/20], Loss: 0.5307\nValidation Accuracy: 83.33%\nEpoch [6/20], Loss: 0.3751\nValidation Accuracy: 88.89%\nEpoch [7/20], Loss: 0.2691\nValidation Accuracy: 77.78%\nEpoch [8/20], Loss: 0.1304\nValidation Accuracy: 75.00%\nEpoch [9/20], Loss: 0.1531\nValidation Accuracy: 86.11%\nEpoch [10/20], Loss: 0.0413\nValidation Accuracy: 88.89%\nEpoch [11/20], Loss: 0.0682\nValidation Accuracy: 86.11%\nEpoch [12/20], Loss: 0.0452\nValidation Accuracy: 86.11%\nEpoch [13/20], Loss: 0.0403\nValidation Accuracy: 86.11%\nEpoch [14/20], Loss: 0.0715\nValidation Accuracy: 86.11%\nEpoch [15/20], Loss: 0.0830\nValidation Accuracy: 86.11%\nEpoch [16/20], Loss: 0.0524\nValidation Accuracy: 86.11%\nEpoch [17/20], Loss: 0.0171\nValidation Accuracy: 86.11%\nEpoch [18/20], Loss: 0.0842\nValidation Accuracy: 86.11%\nEpoch [19/20], Loss: 0.0294\nValidation Accuracy: 86.11%\nEpoch [20/20], Loss: 0.0298\nValidation Accuracy: 86.11%\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        preds = torch.sigmoid(outputs).squeeze() > 0.5\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T21:05:07.667979Z","iopub.execute_input":"2025-05-23T21:05:07.668243Z","iopub.status.idle":"2025-05-23T21:05:07.823933Z","shell.execute_reply.started":"2025-05-23T21:05:07.668223Z","shell.execute_reply":"2025-05-23T21:05:07.823279Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 71.79%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}